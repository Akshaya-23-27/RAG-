{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9fAANHIwhr8whDY4rT/PF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshaya-23-27/RAG-/blob/main/RAG_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2 chat with PDF using RAG pipeline"
      ],
      "metadata": {
        "id": "rFRxLhmwRT4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a Python implementation of a Retrieval-Augmented Generation (RAG) pipeline for interacting with websites. The pipeline uses libraries like BeautifulSoup for web scraping, sentence-transformers for embeddings, FAISS for the vector database, and OpenAI for response generation."
      ],
      "metadata": {
        "id": "U2lCGrGAPSRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementaion"
      ],
      "metadata": {
        "id": "PDLUD3OLPgV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def crawl_and_scrape(url):\n",
        "    \"\"\"\n",
        "    Crawl and scrape content from a website.\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Extract textual content from <p> and <h> tags\n",
        "    paragraphs = soup.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
        "    content = [p.get_text().strip() for p in paragraphs if p.get_text().strip()]\n",
        "    return content\n",
        "\n",
        "def segment_and_embed(content, chunk_size=100):\n",
        "    \"\"\"\n",
        "    Segment content into smaller chunks and compute embeddings.\n",
        "    \"\"\"\n",
        "    chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
        "    embeddings = embedding_model.encode([\" \".join(chunk) for chunk in chunks])\n",
        "    return chunks, embeddings\n",
        "\n",
        "def store_embeddings(chunks, embeddings, url):\n",
        "    \"\"\"\n",
        "    Store embeddings in the vector database with metadata.\n",
        "    \"\"\"\n",
        "    global metadata_store\n",
        "    index.add(embeddings)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        metadata_store.append({\"url\": url, \"chunk\": \" \".join(chunk)})\n",
        "\n",
        "def query_database(user_query, top_k=5):\n",
        "    \"\"\"\n",
        "    Query the vector database and retrieve relevant chunks.\n",
        "    \"\"\"\n",
        "    query_embedding = embedding_model.encode([user_query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    results = [metadata_store[idx] for idx in indices[0] if idx < len(metadata_store)]\n",
        "    return results\n",
        "\n",
        "def generate_response(retrieved_data, user_query):\n",
        "    \"\"\"\n",
        "    Generate a response using OpenAI GPT with retrieved data.\n",
        "    \"\"\"\n",
        "    context = \"\\n\\n\".join([f\"Chunk from {item['url']}:\\n{item['chunk']}\" for item in retrieved_data])\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI assistant with access to the following information:\n",
        "    {context}\n",
        "\n",
        "    User's query: {user_query}\n",
        "    Provide a detailed and accurate response using the above context.\n",
        "    \"\"\"\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=500,\n",
        "        temperature=0\n",
        "    )\n",
        "    return response[\"choices\"][0][\"text\"].strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "e9beFGh0DnkQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Steps Explained\n",
        "\n",
        "1.Data Ingestion:\n",
        "\n",
        "Crawls and scrapes websites using requests and BeautifulSoup. Segments textual data into smaller chunks for granularity. Embedding and Storage:\n",
        "\n",
        "Computes embeddings for each chunk using a SentenceTransformer model. Stores embeddings in FAISS for efficient similarity-based retrieval.\n",
        "\n",
        "2.Query Handling:\n",
        "\n",
        "Embeds the user query and performs a similarity search in the FAISS index. Retrieves the top-k relevant chunks based on similarity scores. Response Generation:\n",
        "\n",
        "Passes retrieved chunks to OpenAI GPT along with the user's query to generate a context-rich response."
      ],
      "metadata": {
        "id": "Vn8fQnY9J5zO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1 : chat with website using RAG pipeline"
      ],
      "metadata": {
        "id": "hJ0JQgiELDPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement a Retrieval-Augmented Generation (RAG) pipeline for chatting with PDFs, you can use the following stack:\n",
        "\n",
        "PDF parsing and chunking: PyPDF2 or pdfplumber for extracting text and logical segmentation.\n",
        "Embeddings: sentence-transformers for generating vector embeddings.\n",
        "Vector database: FAISS, Weaviate, or Pinecone for efficient similarity-based retrieval.\n",
        "LLM integration: OpenAI API or Hugging Face for generating responses.\n",
        "Framework: LangChain or a custom framework to integrate all components."
      ],
      "metadata": {
        "id": "cPbRdaKHLbTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Data Ingestion\n",
        "def ingest_pdfs(pdf_files):\n",
        "    \"\"\"Extract and chunk text from PDFs, and store embeddings in a vector database.\"\"\"\n",
        "    docs = []\n",
        "    for pdf_file in pdf_files:\n",
        "        reader = PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        docs.append(text)\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    chunks = []\n",
        "    for doc in docs:\n",
        "        chunks.extend(text_splitter.split_text(doc))\n",
        "\n",
        "    # Generate embeddings and store in a vector database\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
        "\n",
        "    return vector_store\n",
        "\n",
        "# Step 2: Query Handling\n",
        "def handle_query(vector_store, query, llm_model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"Handle user queries by retrieving relevant data and generating a response.\"\"\"\n",
        "    retriever = vector_store.as_retriever()\n",
        "    qa_chain = RetrievalQA(llm=OpenAI(model=llm_model), retriever=retriever)\n",
        "\n",
        "    response = qa_chain.run(query)\n",
        "    return response\n",
        "\n",
        "# Step 3: Comparison Queries\n",
        "def handle_comparison_query(vector_store, comparison_query):\n",
        "    \"\"\"Perform comparison queries and generate a structured response.\"\"\"\n",
        "    # Retrieve relevant chunks\n",
        "    retriever = vector_store.as_retriever()\n",
        "    docs = retriever.get_relevant_documents(comparison_query)\n",
        "\n",
        "    # Aggregate and process data\n",
        "    comparison_data = {}\n",
        "    for doc in docs:\n",
        "        # Add logic to parse fields for comparison\n",
        "        # For simplicity, we'll assume each document contains structured data\n",
        "        comparison_data[doc.metadata[\"source\"]] = doc.page_content\n",
        "\n",
        "    # Generate a structured response (example: tabular format)\n",
        "    response = \"Comparison:\\n\"\n",
        "    for source, content in comparison_data.items():\n",
        "        response += f\"- Source: {source}\\n  Content: {content}\\n\"\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "duIyn7OqMKIG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of Components\n",
        "\n",
        "1.Data Ingestion:\n",
        "\n",
        ". PDFs are parsed using PyPDF2\n",
        ". Text is chunked into manageable parts using\n",
        " RecursiveCharacterTextSplitter.\n",
        ". Vector embeddings are generated using a pre-trained embedding model (sentence-transformers).\n",
        "\n",
        "2.Query Handling:\n",
        "\n",
        "User queries are embedded and matched with the stored embeddings in the vector database.\n",
        "Relevant chunks are retrieved, and the LLM generates responses.\n",
        "\n",
        "3.Comparison Queries:\n",
        "\n",
        "Specific terms or fields for comparison are identified.\n",
        "Retrieved chunks are processed to generate structured comparisons.\n",
        "\n",
        "4.Response Generation:\n",
        "\n",
        "The LLM produces detailed and natural-language responses using retrieval-augmented context."
      ],
      "metadata": {
        "id": "uOLL_BGjNPVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up OpenAI API key :"
      ],
      "metadata": {
        "id": "5Pa9SECNQe9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"YOUR_OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "Hn2eIR74OhQc"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}